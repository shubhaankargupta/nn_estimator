{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "equation_estimator_pytorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMCJb2m2JH0kH4c3lYNTCZc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubhaankargupta/nn_estimator/blob/main/equation_estimator_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiaJ3S-McpQo"
      },
      "source": [
        "Credits: https://towardsdatascience.com/linear-regression-with-pytorch-eb6dedead817"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKfxNr1CafnV"
      },
      "source": [
        "import numpy as np\n",
        "# create dummy data for training\n",
        "x_values = [i for i in range(5)]\n",
        "x_train = np.array(x_values, dtype=np.float32)\n",
        "x_train = x_train.reshape(-1, 1)\n",
        "\n",
        "\n",
        "\n",
        "#instead of i*i, write any equation that you want\n",
        "y_values = [i*i for i in x_values]\n",
        "y_train = np.array(y_values, dtype=np.float32)\n",
        "y_train = y_train.reshape(-1, 1)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka3l6UlqamoI"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "class linearRegression(torch.nn.Module):\n",
        "    def __init__(self, inputSize, outputSize):\n",
        "        super(linearRegression, self).__init__()\n",
        "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geEE9uX5an5Y"
      },
      "source": [
        "inputDim = 1        # takes variable 'x' \n",
        "outputDim = 1       # takes variable 'y'\n",
        "learningRate = 0.01 \n",
        "epochs = 5000\n",
        "\n",
        "model = linearRegression(inputDim, outputDim)\n",
        "##### For GPU #######\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2huzgwhardZ"
      },
      "source": [
        "criterion = torch.nn.MSELoss() \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dj4BTts5as_O",
        "outputId": "83e8a19c-21b6-4c9d-c8f1-a3d9071ab097"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    # Converting inputs and labels to Variable\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
        "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
        "    else:\n",
        "        inputs = Variable(torch.from_numpy(x_train))\n",
        "        labels = Variable(torch.from_numpy(y_train))\n",
        "\n",
        "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # get output from the model, given the inputs\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # get loss for the predicted output\n",
        "    loss = criterion(outputs, labels)\n",
        "    print(loss)\n",
        "    # get gradients w.r.t to parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2500, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2501, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2502, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2503, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2504, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2505, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2506, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2507, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2508, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2509, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2510, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2511, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2512, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2513, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2514, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2515, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2516, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2517, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2518, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2519, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2520, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2521, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2522, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2523, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2524, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2525, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2526, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2527, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2528, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2529, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2530, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2531, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2532, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2533, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2534, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2535, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2536, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2537, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2538, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2539, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2540, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2541, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2542, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2543, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2544, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2545, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2546, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2547, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2548, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2549, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2550, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2551, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2552, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2553, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2554, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2555, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2556, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2557, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2558, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2559, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2560, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2561, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2562, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2563, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2564, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2565, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2566, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2567, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2568, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2569, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2570, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2571, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2572, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2573, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2574, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2575, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2576, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2577, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2578, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2579, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2580, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2581, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2582, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2583, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2584, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2585, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2586, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2587, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2588, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2589, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2590, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2591, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2592, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2593, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2594, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2595, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2596, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2597, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2598, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2599, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2600, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2601, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2602, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2603, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2604, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2605, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2606, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2607, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2608, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2609, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2610, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2611, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2612, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2613, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2614, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2615, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2616, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2617, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2618, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2619, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2620, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2621, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2622, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2623, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2624, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2625, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2626, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2627, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2628, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2629, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2630, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2631, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2632, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2633, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2634, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2635, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2636, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2637, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2638, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2639, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2640, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2641, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2642, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2643, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2644, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2645, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2646, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2647, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2648, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2649, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2650, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2651, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2652, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2653, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2654, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2655, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2656, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2657, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2658, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2659, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2660, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2661, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2662, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2663, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2664, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2665, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2666, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2667, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2668, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2669, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2670, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2671, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2672, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2673, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2674, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2675, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2676, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2677, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2678, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2679, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2680, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2681, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2682, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2683, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2684, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2685, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2686, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2687, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2688, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2689, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2690, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2691, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2692, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2693, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2694, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2695, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2696, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2697, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2698, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2699, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2700, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2701, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2702, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2703, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2704, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2705, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2706, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2707, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2708, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2709, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2710, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2711, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2712, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2713, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2714, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2715, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2716, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2717, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2718, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2719, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2720, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2721, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2722, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2723, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2724, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2725, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2726, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2727, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2728, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2729, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2730, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2731, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2732, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2733, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2734, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2735, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2736, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2737, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2738, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2739, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2740, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2741, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2742, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2743, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2744, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2745, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2746, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2747, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2748, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2749, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2750, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2751, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2752, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2753, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2754, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2755, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2756, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2757, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2758, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2759, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2760, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2761, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2762, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2763, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2764, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2765, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2766, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2767, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2768, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2769, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2770, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2771, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2772, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2773, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2774, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2775, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2776, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2777, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2778, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2779, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2780, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2781, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2782, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2783, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2784, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2785, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2786, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2787, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2788, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2789, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2790, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2791, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2792, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2793, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2794, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2795, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2796, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2797, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2798, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2799, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2800, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2801, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2802, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2803, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2804, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2805, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2806, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2807, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2808, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2809, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2810, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2811, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2812, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2813, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2814, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2815, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2816, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2817, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2818, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2819, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2820, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2821, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2822, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2823, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2824, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2825, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2826, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2827, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2828, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2829, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2830, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2831, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2832, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2833, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2834, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2835, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2836, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2837, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2838, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2839, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2840, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2841, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2842, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2843, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2844, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2845, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2846, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2847, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2848, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2849, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2850, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2851, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2852, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2853, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2854, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2855, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2856, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2857, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2858, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2859, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2860, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2861, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2862, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2863, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2864, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2865, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2866, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2867, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2868, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2869, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2870, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2871, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2872, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2873, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2874, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2875, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2876, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2877, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2878, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2879, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2880, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2881, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2882, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2883, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2884, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2885, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2886, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2887, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2888, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2889, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2890, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2891, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2892, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2893, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2894, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2895, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2896, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2897, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2898, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2899, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2900, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2901, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2902, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2903, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2904, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2905, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2906, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2907, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2908, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2909, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2910, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2911, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2912, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2913, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2914, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2915, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2916, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2917, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2918, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2919, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2920, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2921, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2922, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2923, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2924, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2925, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2926, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2927, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2928, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2929, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2930, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2931, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2932, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2933, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2934, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2935, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2936, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2937, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2938, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2939, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2940, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2941, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2942, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2943, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2944, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2945, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2946, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2947, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2948, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2949, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2950, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2951, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2952, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2953, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2954, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2955, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2956, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2957, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2958, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2959, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2960, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2961, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2962, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2963, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2964, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2965, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2966, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2967, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2968, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2969, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2970, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2971, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2972, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2973, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2974, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2975, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2976, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2977, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2978, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2979, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2980, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2981, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2982, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2983, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2984, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2985, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2986, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2987, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2988, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2989, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2990, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2991, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2992, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2993, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2994, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2995, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2996, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2997, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2998, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 2999, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3000, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3001, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3002, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3003, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3004, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3005, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3006, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3007, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3008, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3009, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3010, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3011, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3012, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3013, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3014, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3015, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3016, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3017, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3018, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3019, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3020, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3021, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3022, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3023, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3024, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3025, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3026, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3027, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3028, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3029, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3030, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3031, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3032, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3033, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3034, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3035, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3036, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3037, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3038, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3039, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3040, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3041, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3042, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3043, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3044, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3045, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3046, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3047, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3048, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3049, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3050, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3051, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3052, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3053, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3054, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3055, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3056, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3057, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3058, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3059, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3060, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3061, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3062, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3063, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3064, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3065, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3066, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3067, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3068, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3069, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3070, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3071, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3072, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3073, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3074, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3075, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3076, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3077, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3078, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3079, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3080, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3081, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3082, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3083, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3084, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3085, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3086, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3087, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3088, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3089, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3090, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3091, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3092, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3093, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3094, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3095, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3096, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3097, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3098, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3099, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3100, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3101, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3102, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3103, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3104, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3105, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3106, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3107, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3108, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3109, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3110, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3111, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3112, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3113, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3114, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3115, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3116, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3117, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3118, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3119, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3120, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3121, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3122, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3123, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3124, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3125, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3126, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3127, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3128, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3129, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3130, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3131, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3132, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3133, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3134, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3135, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3136, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3137, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3138, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3139, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3140, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3141, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3142, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3143, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3144, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3145, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3146, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3147, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3148, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3149, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3150, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3151, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3152, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3153, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3154, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3155, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3156, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3157, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3158, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3159, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3160, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3161, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3162, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3163, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3164, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3165, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3166, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3167, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3168, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3169, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3170, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3171, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3172, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3173, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3174, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3175, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3176, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3177, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3178, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3179, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3180, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3181, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3182, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3183, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3184, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3185, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3186, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3187, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3188, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3189, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3190, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3191, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3192, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3193, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3194, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3195, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3196, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3197, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3198, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3199, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3200, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3201, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3202, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3203, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3204, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3205, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3206, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3207, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3208, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3209, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3210, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3211, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3212, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3213, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3214, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3215, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3216, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3217, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3218, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3219, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3220, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3221, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3222, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3223, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3224, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3225, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3226, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3227, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3228, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3229, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3230, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3231, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3232, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3233, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3234, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3235, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3236, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3237, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3238, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3239, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3240, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3241, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3242, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3243, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3244, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3245, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3246, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3247, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3248, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3249, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3250, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3251, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3252, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3253, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3254, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3255, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3256, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3257, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3258, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3259, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3260, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3261, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3262, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3263, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3264, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3265, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3266, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3267, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3268, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3269, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3270, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3271, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3272, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3273, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3274, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3275, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3276, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3277, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3278, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3279, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3280, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3281, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3282, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3283, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3284, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3285, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3286, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3287, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3288, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3289, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3290, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3291, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3292, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3293, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3294, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3295, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3296, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3297, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3298, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3299, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3300, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3301, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3302, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3303, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3304, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3305, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3306, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3307, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3308, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3309, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3310, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3311, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3312, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3313, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3314, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3315, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3316, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3317, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3318, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3319, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3320, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3321, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3322, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3323, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3324, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3325, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3326, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3327, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3328, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3329, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3330, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3331, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3332, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3333, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3334, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3335, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3336, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3337, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3338, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3339, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3340, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3341, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3342, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3343, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3344, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3345, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3346, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3347, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3348, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3349, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3350, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3351, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3352, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3353, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3354, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3355, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3356, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3357, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3358, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3359, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3360, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3361, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3362, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3363, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3364, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3365, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3366, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3367, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3368, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3369, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3370, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3371, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3372, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3373, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3374, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3375, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3376, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3377, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3378, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3379, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3380, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3381, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3382, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3383, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3384, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3385, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3386, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3387, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3388, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3389, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3390, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3391, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3392, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3393, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3394, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3395, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3396, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3397, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3398, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3399, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3400, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3401, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3402, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3403, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3404, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3405, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3406, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3407, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3408, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3409, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3410, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3411, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3412, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3413, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3414, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3415, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3416, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3417, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3418, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3419, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3420, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3421, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3422, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3423, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3424, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3425, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3426, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3427, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3428, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3429, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3430, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3431, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3432, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3433, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3434, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3435, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3436, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3437, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3438, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3439, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3440, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3441, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3442, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3443, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3444, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3445, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3446, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3447, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3448, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3449, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3450, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3451, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3452, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3453, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3454, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3455, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3456, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3457, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3458, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3459, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3460, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3461, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3462, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3463, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3464, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3465, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3466, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3467, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3468, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3469, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3470, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3471, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3472, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3473, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3474, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3475, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3476, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3477, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3478, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3479, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3480, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3481, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3482, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3483, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3484, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3485, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3486, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3487, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3488, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3489, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3490, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3491, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3492, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3493, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3494, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3495, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3496, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3497, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3498, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3499, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3500, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3501, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3502, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3503, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3504, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3505, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3506, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3507, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3508, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3509, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3510, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3511, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3512, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3513, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3514, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3515, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3516, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3517, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3518, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3519, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3520, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3521, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3522, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3523, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3524, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3525, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3526, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3527, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3528, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3529, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3530, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3531, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3532, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3533, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3534, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3535, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3536, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3537, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3538, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3539, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3540, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3541, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3542, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3543, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3544, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3545, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3546, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3547, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3548, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3549, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3550, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3551, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3552, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3553, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3554, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3555, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3556, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3557, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3558, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3559, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3560, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3561, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3562, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3563, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3564, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3565, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3566, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3567, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3568, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3569, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3570, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3571, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3572, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3573, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3574, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3575, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3576, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3577, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3578, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3579, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3580, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3581, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3582, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3583, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3584, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3585, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3586, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3587, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3588, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3589, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3590, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3591, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3592, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3593, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3594, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3595, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3596, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3597, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3598, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3599, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3600, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3601, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3602, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3603, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3604, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3605, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3606, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3607, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3608, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3609, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3610, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3611, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3612, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3613, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3614, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3615, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3616, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3617, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3618, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3619, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3620, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3621, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3622, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3623, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3624, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3625, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3626, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3627, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3628, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3629, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3630, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3631, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3632, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3633, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3634, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3635, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3636, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3637, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3638, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3639, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3640, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3641, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3642, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3643, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3644, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3645, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3646, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3647, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3648, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3649, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3650, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3651, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3652, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3653, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3654, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3655, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3656, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3657, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3658, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3659, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3660, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3661, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3662, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3663, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3664, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3665, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3666, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3667, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3668, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3669, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3670, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3671, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3672, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3673, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3674, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3675, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3676, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3677, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3678, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3679, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3680, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3681, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3682, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3683, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3684, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3685, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3686, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3687, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3688, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3689, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3690, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3691, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3692, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3693, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3694, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3695, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3696, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3697, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3698, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3699, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3700, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3701, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3702, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3703, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3704, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3705, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3706, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3707, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3708, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3709, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3710, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3711, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3712, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3713, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3714, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3715, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3716, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3717, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3718, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3719, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3720, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3721, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3722, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3723, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3724, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3725, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3726, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3727, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3728, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3729, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3730, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3731, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3732, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3733, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3734, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3735, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3736, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3737, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3738, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3739, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3740, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3741, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3742, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3743, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3744, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3745, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3746, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3747, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3748, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3749, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3750, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3751, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3752, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3753, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3754, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3755, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3756, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3757, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3758, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3759, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3760, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3761, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3762, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3763, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3764, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3765, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3766, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3767, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3768, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3769, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3770, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3771, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3772, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3773, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3774, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3775, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3776, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3777, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3778, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3779, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3780, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3781, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3782, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3783, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3784, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3785, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3786, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3787, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3788, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3789, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3790, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3791, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3792, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3793, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3794, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3795, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3796, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3797, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3798, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3799, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3800, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3801, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3802, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3803, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3804, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3805, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3806, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3807, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3808, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3809, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3810, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3811, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3812, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3813, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3814, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3815, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3816, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3817, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3818, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3819, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3820, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3821, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3822, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3823, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3824, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3825, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3826, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3827, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3828, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3829, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3830, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3831, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3832, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3833, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3834, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3835, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3836, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3837, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3838, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3839, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3840, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3841, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3842, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3843, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3844, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3845, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3846, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3847, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3848, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3849, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3850, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3851, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3852, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3853, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3854, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3855, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3856, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3857, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3858, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3859, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3860, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3861, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3862, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3863, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3864, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3865, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3866, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3867, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3868, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3869, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3870, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3871, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3872, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3873, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3874, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3875, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3876, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3877, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3878, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3879, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3880, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3881, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3882, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3883, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3884, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3885, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3886, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3887, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3888, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3889, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3890, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3891, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3892, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3893, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3894, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3895, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3896, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3897, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3898, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3899, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3900, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3901, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3902, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3903, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3904, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3905, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3906, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3907, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3908, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3909, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3910, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3911, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3912, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3913, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3914, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3915, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3916, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3917, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3918, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3919, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3920, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3921, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3922, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3923, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3924, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3925, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3926, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3927, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3928, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3929, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3930, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3931, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3932, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3933, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3934, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3935, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3936, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3937, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3938, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3939, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3940, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3941, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3942, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3943, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3944, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3945, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3946, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3947, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3948, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3949, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3950, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3951, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3952, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3953, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3954, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3955, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3956, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3957, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3958, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3959, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3960, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3961, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3962, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3963, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3964, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3965, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3966, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3967, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3968, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3969, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3970, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3971, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3972, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3973, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3974, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3975, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3976, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3977, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3978, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3979, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3980, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3981, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3982, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3983, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3984, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3985, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3986, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3987, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3988, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3989, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3990, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3991, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3992, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3993, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3994, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3995, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3996, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3997, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3998, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 3999, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4000, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4001, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4002, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4003, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4004, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4005, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4006, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4007, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4008, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4009, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4010, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4011, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4012, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4013, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4014, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4015, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4016, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4017, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4018, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4019, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4020, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4021, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4022, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4023, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4024, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4025, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4026, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4027, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4028, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4029, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4030, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4031, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4032, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4033, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4034, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4035, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4036, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4037, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4038, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4039, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4040, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4041, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4042, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4043, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4044, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4045, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4046, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4047, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4048, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4049, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4050, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4051, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4052, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4053, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4054, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4055, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4056, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4057, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4058, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4059, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4060, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4061, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4062, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4063, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4064, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4065, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4066, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4067, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4068, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4069, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4070, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4071, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4072, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4073, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4074, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4075, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4076, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4077, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4078, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4079, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4080, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4081, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4082, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4083, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4084, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4085, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4086, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4087, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4088, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4089, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4090, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4091, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4092, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4093, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4094, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4095, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4096, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4097, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4098, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4099, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4100, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4101, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4102, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4103, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4104, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4105, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4106, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4107, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4108, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4109, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4110, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4111, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4112, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4113, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4114, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4115, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4116, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4117, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4118, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4119, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4120, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4121, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4122, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4123, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4124, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4125, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4126, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4127, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4128, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4129, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4130, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4131, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4132, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4133, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4134, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4135, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4136, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4137, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4138, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4139, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4140, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4141, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4142, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4143, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4144, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4145, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4146, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4147, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4148, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4149, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4150, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4151, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4152, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4153, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4154, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4155, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4156, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4157, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4158, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4159, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4160, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4161, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4162, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4163, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4164, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4165, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4166, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4167, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4168, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4169, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4170, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4171, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4172, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4173, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4174, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4175, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4176, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4177, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4178, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4179, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4180, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4181, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4182, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4183, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4184, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4185, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4186, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4187, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4188, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4189, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4190, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4191, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4192, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4193, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4194, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4195, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4196, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4197, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4198, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4199, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4200, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4201, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4202, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4203, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4204, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4205, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4206, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4207, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4208, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4209, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4210, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4211, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4212, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4213, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4214, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4215, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4216, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4217, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4218, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4219, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4220, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4221, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4222, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4223, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4224, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4225, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4226, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4227, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4228, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4229, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4230, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4231, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4232, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4233, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4234, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4235, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4236, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4237, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4238, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4239, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4240, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4241, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4242, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4243, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4244, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4245, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4246, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4247, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4248, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4249, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4250, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4251, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4252, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4253, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4254, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4255, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4256, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4257, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4258, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4259, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4260, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4261, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4262, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4263, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4264, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4265, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4266, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4267, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4268, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4269, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4270, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4271, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4272, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4273, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4274, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4275, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4276, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4277, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4278, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4279, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4280, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4281, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4282, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4283, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4284, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4285, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4286, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4287, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4288, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4289, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4290, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4291, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4292, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4293, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4294, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4295, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4296, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4297, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4298, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4299, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4300, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4301, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4302, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4303, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4304, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4305, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4306, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4307, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4308, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4309, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4310, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4311, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4312, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4313, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4314, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4315, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4316, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4317, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4318, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4319, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4320, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4321, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4322, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4323, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4324, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4325, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4326, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4327, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4328, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4329, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4330, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4331, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4332, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4333, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4334, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4335, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4336, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4337, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4338, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4339, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4340, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4341, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4342, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4343, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4344, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4345, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4346, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4347, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4348, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4349, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4350, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4351, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4352, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4353, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4354, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4355, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4356, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4357, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4358, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4359, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4360, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4361, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4362, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4363, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4364, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4365, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4366, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4367, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4368, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4369, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4370, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4371, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4372, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4373, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4374, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4375, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4376, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4377, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4378, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4379, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4380, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4381, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4382, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4383, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4384, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4385, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4386, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4387, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4388, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4389, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4390, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4391, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4392, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4393, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4394, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4395, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4396, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4397, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4398, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4399, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4400, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4401, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4402, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4403, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4404, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4405, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4406, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4407, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4408, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4409, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4410, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4411, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4412, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4413, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4414, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4415, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4416, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4417, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4418, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4419, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4420, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4421, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4422, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4423, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4424, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4425, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4426, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4427, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4428, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4429, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4430, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4431, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4432, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4433, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4434, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4435, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4436, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4437, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4438, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4439, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4440, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4441, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4442, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4443, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4444, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4445, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4446, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4447, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4448, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4449, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4450, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4451, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4452, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4453, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4454, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4455, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4456, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4457, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4458, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4459, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4460, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4461, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4462, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4463, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4464, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4465, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4466, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4467, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4468, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4469, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4470, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4471, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4472, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4473, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4474, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4475, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4476, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4477, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4478, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4479, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4480, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4481, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4482, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4483, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4484, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4485, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4486, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4487, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4488, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4489, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4490, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4491, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4492, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4493, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4494, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4495, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4496, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4497, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4498, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4499, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4500, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4501, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4502, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4503, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4504, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4505, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4506, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4507, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4508, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4509, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4510, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4511, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4512, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4513, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4514, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4515, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4516, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4517, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4518, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4519, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4520, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4521, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4522, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4523, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4524, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4525, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4526, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4527, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4528, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4529, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4530, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4531, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4532, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4533, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4534, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4535, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4536, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4537, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4538, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4539, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4540, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4541, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4542, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4543, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4544, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4545, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4546, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4547, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4548, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4549, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4550, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4551, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4552, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4553, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4554, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4555, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4556, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4557, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4558, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4559, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4560, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4561, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4562, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4563, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4564, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4565, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4566, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4567, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4568, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4569, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4570, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4571, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4572, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4573, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4574, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4575, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4576, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4577, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4578, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4579, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4580, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4581, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4582, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4583, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4584, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4585, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4586, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4587, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4588, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4589, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4590, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4591, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4592, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4593, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4594, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4595, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4596, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4597, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4598, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4599, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4600, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4601, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4602, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4603, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4604, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4605, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4606, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4607, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4608, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4609, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4610, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4611, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4612, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4613, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4614, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4615, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4616, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4617, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4618, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4619, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4620, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4621, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4622, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4623, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4624, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4625, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4626, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4627, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4628, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4629, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4630, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4631, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4632, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4633, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4634, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4635, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4636, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4637, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4638, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4639, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4640, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4641, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4642, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4643, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4644, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4645, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4646, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4647, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4648, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4649, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4650, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4651, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4652, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4653, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4654, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4655, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4656, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4657, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4658, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4659, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4660, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4661, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4662, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4663, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4664, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4665, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4666, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4667, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4668, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4669, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4670, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4671, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4672, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4673, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4674, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4675, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4676, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4677, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4678, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4679, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4680, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4681, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4682, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4683, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4684, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4685, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4686, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4687, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4688, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4689, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4690, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4691, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4692, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4693, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4694, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4695, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4696, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4697, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4698, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4699, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4700, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4701, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4702, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4703, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4704, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4705, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4706, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4707, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4708, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4709, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4710, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4711, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4712, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4713, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4714, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4715, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4716, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4717, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4718, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4719, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4720, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4721, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4722, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4723, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4724, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4725, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4726, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4727, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4728, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4729, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4730, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4731, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4732, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4733, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4734, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4735, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4736, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4737, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4738, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4739, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4740, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4741, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4742, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4743, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4744, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4745, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4746, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4747, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4748, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4749, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4750, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4751, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4752, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4753, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4754, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4755, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4756, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4757, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4758, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4759, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4760, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4761, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4762, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4763, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4764, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4765, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4766, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4767, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4768, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4769, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4770, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4771, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4772, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4773, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4774, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4775, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4776, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4777, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4778, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4779, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4780, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4781, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4782, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4783, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4784, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4785, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4786, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4787, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4788, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4789, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4790, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4791, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4792, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4793, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4794, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4795, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4796, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4797, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4798, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4799, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4800, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4801, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4802, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4803, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4804, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4805, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4806, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4807, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4808, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4809, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4810, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4811, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4812, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4813, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4814, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4815, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4816, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4817, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4818, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4819, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4820, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4821, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4822, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4823, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4824, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4825, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4826, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4827, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4828, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4829, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4830, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4831, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4832, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4833, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4834, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4835, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4836, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4837, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4838, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4839, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4840, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4841, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4842, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4843, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4844, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4845, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4846, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4847, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4848, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4849, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4850, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4851, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4852, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4853, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4854, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4855, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4856, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4857, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4858, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4859, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4860, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4861, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4862, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4863, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4864, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4865, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4866, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4867, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4868, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4869, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4870, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4871, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4872, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4873, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4874, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4875, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4876, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4877, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4878, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4879, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4880, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4881, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4882, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4883, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4884, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4885, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4886, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4887, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4888, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4889, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4890, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4891, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4892, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4893, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4894, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4895, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4896, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4897, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4898, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4899, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4900, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4901, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4902, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4903, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4904, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4905, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4906, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4907, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4908, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4909, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4910, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4911, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4912, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4913, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4914, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4915, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4916, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4917, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4918, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4919, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4920, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4921, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4922, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4923, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4924, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4925, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4926, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4927, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4928, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4929, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4930, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4931, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4932, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4933, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4934, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4935, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4936, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4937, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4938, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4939, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4940, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4941, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4942, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4943, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4944, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4945, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4946, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4947, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4948, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4949, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4950, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4951, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4952, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4953, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4954, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4955, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4956, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4957, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4958, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4959, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4960, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4961, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4962, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4963, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4964, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4965, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4966, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4967, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4968, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4969, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4970, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4971, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4972, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4973, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4974, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4975, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4976, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4977, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4978, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4979, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4980, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4981, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4982, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4983, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4984, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4985, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4986, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4987, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4988, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4989, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4990, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4991, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4992, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4993, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4994, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4995, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4996, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4997, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4998, loss 2.799999952316284\n",
            "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
            "epoch 4999, loss 2.799999952316284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "-p7SN_jqatmz",
        "outputId": "d9f731f3-be3e-4272-e9ef-e240d716884f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "with torch.no_grad(): # we don't need gradients in the testing phase\n",
        "    if torch.cuda.is_available():\n",
        "        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n",
        "    else:\n",
        "        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
        "    print(predicted)\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\n",
        "plt.plot(x_train, predicted, '--', label='Predictions', alpha=0.5)\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.9999855]\n",
            " [ 2.0000088]\n",
            " [ 6.000003 ]\n",
            " [ 9.999997 ]\n",
            " [13.999991 ]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}